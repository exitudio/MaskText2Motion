{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Bidrec Autoregress\n",
    "from bidirec_autoregress import dataset_TM_train\n",
    "import bidirec_autoregress.eval_trans as eval_trans\n",
    "import bidirec_autoregress.t2m_trans as trans\n",
    "from bidirec_autoregress.util import get_bidirec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "# import utils.eval_trans as eval_trans\n",
    "# from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "# import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:06<00:00, 3505.56it/s]\n",
      "100%|██████████| 1460/1460 [00:00<00:00, 2977.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 't2m'\n",
    "args.down_t = 2\n",
    "\n",
    "train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)\n",
    "\n",
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        return self.model.encode_text(text)\n",
    "clip_model = TextCLIP(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nb_code = 512 # 8192 # \n",
    "args.code_dim = 512 # 32 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.embed_dim_gpt = 1024\n",
    "args.clip_dim = 512\n",
    "args.block_size = 49\n",
    "args.num_layers = 9\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "\n",
    "args.vq_name = 'VQVAE_official_last'\n",
    "args.exp_name = 'TEMP'\n",
    "args.out_dir = 'output_GPT_Final'\n",
    "args.out_dir = os.path.join(args.out_dir, f'{args.exp_name}')\n",
    "args.vq_dir= f'output/{args.vq_name}'\n",
    "codebook_dir = f'{args.vq_dir}/codebook/'\n",
    "os.makedirs(args.out_dir, exist_ok = True)\n",
    "os.makedirs(args.vq_dir, exist_ok = True)\n",
    "os.makedirs(codebook_dir, exist_ok = True)\n",
    "\n",
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate)\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                            embed_dim=args.embed_dim_gpt, \n",
    "                                            clip_dim=args.clip_dim, \n",
    "                                            block_size=args.block_size, \n",
    "                                            num_layers=args.num_layers, \n",
    "                                            n_head=args.n_head_gpt, \n",
    "                                            drop_out_rate=args.drop_out_rate, \n",
    "                                            fc_rate=args.ff_rate)\n",
    "args.resume_pth = '/home/epinyoan/git/MaskText2Motion/T2M-GPT-Batch/output/VQVAE_official_last/net_last.pth'\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "\n",
    "args.resume_trans = '/home/epinyoan/git/MaskText2Motion/T2M-GPT-Batch/output/2023-05-03-16-01-13_HML3D_3_BidirAutoreg_big/net_last.pth'\n",
    "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder = torch.nn.DataParallel(trans_encoder)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()\n",
    "''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE encoder index precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader_token:\n",
    "    pose, name = batch\n",
    "    bs, seq = pose.shape[0], pose.shape[1]\n",
    "\n",
    "    pose = pose.cuda().float() # bs, nb_joints, joints_dim, seq_len\n",
    "    target = net(pose, type='encode')\n",
    "    target = target.cpu().numpy()\n",
    "    np.save(pjoin(codebook_dir, name[0] +'.npy'), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## count used indices\n",
    "# all_indices = None\n",
    "# for batch in train_loader_token:\n",
    "#     pose, name = batch\n",
    "#     pose = pose.cuda().float()\n",
    "    \n",
    "#     target = net(pose, type='encode').squeeze()\n",
    "#     if all_indices is None:\n",
    "#         all_indices = target\n",
    "#     else:\n",
    "#         all_indices = torch.cat([all_indices, target])\n",
    "# c = torch.bincount(all_indices.view(-1))\n",
    "# (c==0).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:05<00:00, 4138.56it/s]\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 2\n",
    "num_workers = 8\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t, num_workers=num_workers)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional AutoRegressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearange\n",
    "# 1. taget\n",
    "# 2. input (called in 1. training & 2. sampling)\n",
    "# 3. mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1, 100, 101, 104,  -2, 513, 513, 513],\n",
       "        [ -1, 100, 101, 104, 105,  -2, 513, 513],\n",
       "        [ -1, 100, 101, 102, 105, 106,  -2, 512],\n",
       "        [ -1,  -2, 512, 512, 512, 512, 512, 512]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([[100, 101, 102, 103, 104, 512, 513, 513],\n",
    "                        [100, 101, 102, 103, 104, 105, 512,513],\n",
    "                        [100, 101, 102, 103, 104, 105, 106, 512],\n",
    "                        [512, 512, 512, 512, 512, 512, 512, 512]]).cuda()\n",
    "m_tokens_len = torch.tensor([5, 6, 7, 2]).cuda()\n",
    "# get_bidirec_input(target, m_tokens_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.1443, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pkeep = .5\n",
    "batch = next(train_loader_iter)\n",
    "clip_text, m_tokens, m_tokens_len = batch\n",
    "m_tokens, m_tokens_len = m_tokens.cuda(), m_tokens_len.cuda()\n",
    "bs = m_tokens.shape[0]\n",
    "target = m_tokens    # (bs, 26)\n",
    "target = target.cuda()\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    \n",
    "feat_clip_text = clip_model(text).float()\n",
    "\n",
    "input_index = target #[:,:-1]\n",
    "\n",
    "if args.pkeep == -1:\n",
    "    proba = np.random.rand(1)[0]\n",
    "    mask = torch.bernoulli(proba * torch.ones(input_index.shape,\n",
    "                                                        device=input_index.device))\n",
    "else:\n",
    "    mask = torch.bernoulli(args.pkeep * torch.ones(input_index.shape,\n",
    "                                                        device=input_index.device))\n",
    "# random only motion token (not pad token). To prevent pad token got mixed up.\n",
    "seq_mask_no_end = generate_src_mask(target.shape[1], m_tokens_len)\n",
    "mask = torch.logical_or(mask, ~seq_mask_no_end).int()\n",
    "r_indices = torch.randint_like(input_index, args.nb_code)\n",
    "a_indices = mask*input_index+(1-mask)*r_indices\n",
    "a_indices = get_bidirec_input(a_indices, m_tokens_len)\n",
    "\n",
    "cls_pred = trans_encoder(a_indices, feat_clip_text, m_tokens_len, target.shape[-1])\n",
    "\n",
    "###### Compute Batch Xent ######\n",
    "loss_ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "cb_idx_mask = generate_src_mask(target.shape[1], m_tokens_len)\n",
    "cls_pred_all_masked = torch.masked_select(cls_pred, cb_idx_mask.unsqueeze(-1)).view(-1, cls_pred.shape[-1])\n",
    "target_all_masked = torch.masked_select(target, cb_idx_mask)\n",
    "\n",
    "denom = torch.ones(*target.shape).cuda() * bs * (m_tokens_len).unsqueeze(-1)\n",
    "denom = torch.masked_select(denom, cb_idx_mask)\n",
    "\n",
    "loss_cls = loss_ce(cls_pred_all_masked, target_all_masked) / denom\n",
    "loss_cls = loss_cls.sum()\n",
    "loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_encoder(a_indices, feat_clip_text, m_tokens_len, target.shape[-1])\n",
    "target.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7fc3891d30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = torch.tensor([[10, 11, 12, 13, 14, 512, 512],\n",
    "                        [10, 11, 12, 13, 14, 15, 512],\n",
    "                        [10, 11, 12, 13, 14, 15, 16]]).cuda()\n",
    "m_tokens_len = torch.tensor([5,6,7]).cuda()\n",
    "n_head = 2\n",
    "\n",
    "# all_mask = trans.get_attn_mask(m_tokens_len, target.shape[1], n_head)\n",
    "temp = trans.get_attn_mask(m_tokens_len, target.shape[1], n_head)\n",
    "plt.imshow(temp[1, 0].cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([[100, 101, 102, 103, 104, 512, 513, 513],\n",
    "                        [100, 101, 102, 103, 104, 105, 512,513],\n",
    "                        [100, 101, 102, 103, 104, 105, 106, 512],\n",
    "                        [100, 101, 102, 103, 104, 105, 106, 512]]).cuda()\n",
    "m_tokens_len = torch.tensor([5, 6, 7, 7]).cuda()\n",
    "text = clip.tokenize(['a', 'b', 'c', 'd'], truncate=True).cuda()\n",
    "feat_clip_text = clip_model(text).float()\n",
    "\n",
    "index_motion = trans_encoder(feat_clip_text, m_tokens_len, target.shape[-1], False, type=\"sample\")\n",
    "index_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_steps = torch.floor(m_tokens_len.max()/2)\n",
    "max_steps.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 2949.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 51]),\n",
       " tensor([51.,  5., 51., 51.,  9., 42., 51.,  1.], device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Mask Motion in Eval ####################\n",
    "index_motion = torch.randint(0, 513, (8, 51)).cuda()\n",
    "blank_id = 512 # get_model(trans_encoder).num_vq\n",
    "index_motion[1, 5] = blank_id\n",
    "index_motion[1, 7] = blank_id\n",
    "index_motion[4, 9] = blank_id\n",
    "index_motion[0, 0] = blank_id\n",
    "index_motion[-1, 1] = blank_id\n",
    "\n",
    "# [INFO] 1. this get the last index of blank_id\n",
    "# pred_length = (index_motion == blank_id).int().argmax(1).float()\n",
    "# [INFO] 2. this get the first index of blank_id\n",
    "pred_length = (index_motion >= blank_id).int()\n",
    "pred_length = torch.topk(pred_length, k=1, dim=1).indices.squeeze().float()\n",
    "pred_length[pred_length==0] = index_motion.shape[1]\n",
    "\n",
    "index_motion.shape, pred_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose_eval, pose, m_length, clip_text, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, multimodality, writer, logger = \\\n",
    "    eval_trans.evaluation_transformer(args.out_dir, \n",
    "        val_loader, \n",
    "        net, \n",
    "        trans_encoder, \n",
    "        logger, \n",
    "        writer, \n",
    "        0, \n",
    "        best_fid=1000, \n",
    "        best_iter=0, \n",
    "        best_div=100, \n",
    "        best_top1=0, \n",
    "        best_top2=0, \n",
    "        best_top3=0, \n",
    "        best_matching=100, \n",
    "        clip_model=clip_model, \n",
    "        eval_wrapper=eval_wrapper,\n",
    "        num_repeat=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
