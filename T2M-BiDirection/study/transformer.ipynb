{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.VQVAE_transformer import VQVAETransformer\n",
    "from exit.motiontransformer import MotionTransformerOnly2, generate_src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:02<00:00, 2227.57it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1830.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Reading checkpoints/kit/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 30) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 'kit'\n",
    "args.down_t = 2\n",
    "\n",
    "train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)\n",
    "\n",
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VQVAETransformer:\n\tMissing key(s) in state_dict: \"encoder.sequence_embedding\", \"encoder.joint_embed.weight\", \"encoder.joint_embed.bias\", \"encoder.temporal_blocks.0.norm1.weight\", \"encoder.temporal_blocks.0.norm1.bias\", \"encoder.temporal_blocks.0.attn.qkv.weight\", \"encoder.temporal_blocks.0.attn.qkv.bias\", \"encoder.temporal_blocks.0.attn.proj.weight\", \"encoder.temporal_blocks.0.attn.proj.bias\", \"encoder.temporal_blocks.0.norm2.weight\", \"encoder.temporal_blocks.0.norm2.bias\", \"encoder.temporal_blocks.0.mlp.fc1.weight\", \"encoder.temporal_blocks.0.mlp.fc1.bias\", \"encoder.temporal_blocks.0.mlp.fc2.weight\", \"encoder.temporal_blocks.0.mlp.fc2.bias\", \"encoder.temporal_blocks.1.norm1.weight\", \"encoder.temporal_blocks.1.norm1.bias\", \"encoder.temporal_blocks.1.attn.qkv.weight\", \"encoder.temporal_blocks.1.attn.qkv.bias\", \"encoder.temporal_blocks.1.attn.proj.weight\", \"encoder.temporal_blocks.1.attn.proj.bias\", \"encoder.temporal_blocks.1.norm2.weight\", \"encoder.temporal_blocks.1.norm2.bias\", \"encoder.temporal_blocks.1.mlp.fc1.weight\", \"encoder.temporal_blocks.1.mlp.fc1.bias\", \"encoder.temporal_blocks.1.mlp.fc2.weight\", \"encoder.temporal_blocks.1.mlp.fc2.bias\", \"encoder.temporal_blocks.2.norm1.weight\", \"encoder.temporal_blocks.2.norm1.bias\", \"encoder.temporal_blocks.2.attn.qkv.weight\", \"encoder.temporal_blocks.2.attn.qkv.bias\", \"encoder.temporal_blocks.2.attn.proj.weight\", \"encoder.temporal_blocks.2.attn.proj.bias\", \"encoder.temporal_blocks.2.norm2.weight\", \"encoder.temporal_blocks.2.norm2.bias\", \"encoder.temporal_blocks.2.mlp.fc1.weight\", \"encoder.temporal_blocks.2.mlp.fc1.bias\", \"encoder.temporal_blocks.2.mlp.fc2.weight\", \"encoder.temporal_blocks.2.mlp.fc2.bias\", \"encoder.temporal_blocks.3.norm1.weight\", \"encoder.temporal_blocks.3.norm1.bias\", \"encoder.temporal_blocks.3.attn.qkv.weight\", \"encoder.temporal_blocks.3.attn.qkv.bias\", \"encoder.temporal_blocks.3.attn.proj.weight\", \"encoder.temporal_blocks.3.attn.proj.bias\", \"encoder.temporal_blocks.3.norm2.weight\", \"encoder.temporal_blocks.3.norm2.bias\", \"encoder.temporal_blocks.3.mlp.fc1.weight\", \"encoder.temporal_blocks.3.mlp.fc1.bias\", \"encoder.temporal_blocks.3.mlp.fc2.weight\", \"encoder.temporal_blocks.3.mlp.fc2.bias\", \"encoder.ln_out.weight\", \"encoder.ln_out.bias\", \"encoder.out.weight\", \"encoder.out.bias\", \"decoder.sequence_embedding\", \"decoder.joint_embed.weight\", \"decoder.joint_embed.bias\", \"decoder.temporal_blocks.0.norm1.weight\", \"decoder.temporal_blocks.0.norm1.bias\", \"decoder.temporal_blocks.0.attn.qkv.weight\", \"decoder.temporal_blocks.0.attn.qkv.bias\", \"decoder.temporal_blocks.0.attn.proj.weight\", \"decoder.temporal_blocks.0.attn.proj.bias\", \"decoder.temporal_blocks.0.norm2.weight\", \"decoder.temporal_blocks.0.norm2.bias\", \"decoder.temporal_blocks.0.mlp.fc1.weight\", \"decoder.temporal_blocks.0.mlp.fc1.bias\", \"decoder.temporal_blocks.0.mlp.fc2.weight\", \"decoder.temporal_blocks.0.mlp.fc2.bias\", \"decoder.temporal_blocks.1.norm1.weight\", \"decoder.temporal_blocks.1.norm1.bias\", \"decoder.temporal_blocks.1.attn.qkv.weight\", \"decoder.temporal_blocks.1.attn.qkv.bias\", \"decoder.temporal_blocks.1.attn.proj.weight\", \"decoder.temporal_blocks.1.attn.proj.bias\", \"decoder.temporal_blocks.1.norm2.weight\", \"decoder.temporal_blocks.1.norm2.bias\", \"decoder.temporal_blocks.1.mlp.fc1.weight\", \"decoder.temporal_blocks.1.mlp.fc1.bias\", \"decoder.temporal_blocks.1.mlp.fc2.weight\", \"decoder.temporal_blocks.1.mlp.fc2.bias\", \"decoder.temporal_blocks.2.norm1.weight\", \"decoder.temporal_blocks.2.norm1.bias\", \"decoder.temporal_blocks.2.attn.qkv.weight\", \"decoder.temporal_blocks.2.attn.qkv.bias\", \"decoder.temporal_blocks.2.attn.proj.weight\", \"decoder.temporal_blocks.2.attn.proj.bias\", \"decoder.temporal_blocks.2.norm2.weight\", \"decoder.temporal_blocks.2.norm2.bias\", \"decoder.temporal_blocks.2.mlp.fc1.weight\", \"decoder.temporal_blocks.2.mlp.fc1.bias\", \"decoder.temporal_blocks.2.mlp.fc2.weight\", \"decoder.temporal_blocks.2.mlp.fc2.bias\", \"decoder.temporal_blocks.3.norm1.weight\", \"decoder.temporal_blocks.3.norm1.bias\", \"decoder.temporal_blocks.3.attn.qkv.weight\", \"decoder.temporal_blocks.3.attn.qkv.bias\", \"decoder.temporal_blocks.3.attn.proj.weight\", \"decoder.temporal_blocks.3.attn.proj.bias\", \"decoder.temporal_blocks.3.norm2.weight\", \"decoder.temporal_blocks.3.norm2.bias\", \"decoder.temporal_blocks.3.mlp.fc1.weight\", \"decoder.temporal_blocks.3.mlp.fc1.bias\", \"decoder.temporal_blocks.3.mlp.fc2.weight\", \"decoder.temporal_blocks.3.mlp.fc2.bias\", \"decoder.ln_out.weight\", \"decoder.ln_out.bias\", \"decoder.out.weight\", \"decoder.out.bias\", \"quantizer.codebook\". \n\tUnexpected key(s) in state_dict: \"vqvae.encoder.model.0.weight\", \"vqvae.encoder.model.0.bias\", \"vqvae.encoder.model.2.0.weight\", \"vqvae.encoder.model.2.0.bias\", \"vqvae.encoder.model.2.1.model.0.conv1.weight\", \"vqvae.encoder.model.2.1.model.0.conv1.bias\", \"vqvae.encoder.model.2.1.model.0.conv2.weight\", \"vqvae.encoder.model.2.1.model.0.conv2.bias\", \"vqvae.encoder.model.2.1.model.1.conv1.weight\", \"vqvae.encoder.model.2.1.model.1.conv1.bias\", \"vqvae.encoder.model.2.1.model.1.conv2.weight\", \"vqvae.encoder.model.2.1.model.1.conv2.bias\", \"vqvae.encoder.model.2.1.model.2.conv1.weight\", \"vqvae.encoder.model.2.1.model.2.conv1.bias\", \"vqvae.encoder.model.2.1.model.2.conv2.weight\", \"vqvae.encoder.model.2.1.model.2.conv2.bias\", \"vqvae.encoder.model.3.0.weight\", \"vqvae.encoder.model.3.0.bias\", \"vqvae.encoder.model.3.1.model.0.conv1.weight\", \"vqvae.encoder.model.3.1.model.0.conv1.bias\", \"vqvae.encoder.model.3.1.model.0.conv2.weight\", \"vqvae.encoder.model.3.1.model.0.conv2.bias\", \"vqvae.encoder.model.3.1.model.1.conv1.weight\", \"vqvae.encoder.model.3.1.model.1.conv1.bias\", \"vqvae.encoder.model.3.1.model.1.conv2.weight\", \"vqvae.encoder.model.3.1.model.1.conv2.bias\", \"vqvae.encoder.model.3.1.model.2.conv1.weight\", \"vqvae.encoder.model.3.1.model.2.conv1.bias\", \"vqvae.encoder.model.3.1.model.2.conv2.weight\", \"vqvae.encoder.model.3.1.model.2.conv2.bias\", \"vqvae.encoder.model.4.weight\", \"vqvae.encoder.model.4.bias\", \"vqvae.decoder.model.0.weight\", \"vqvae.decoder.model.0.bias\", \"vqvae.decoder.model.2.0.model.0.conv1.weight\", \"vqvae.decoder.model.2.0.model.0.conv1.bias\", \"vqvae.decoder.model.2.0.model.0.conv2.weight\", \"vqvae.decoder.model.2.0.model.0.conv2.bias\", \"vqvae.decoder.model.2.0.model.1.conv1.weight\", \"vqvae.decoder.model.2.0.model.1.conv1.bias\", \"vqvae.decoder.model.2.0.model.1.conv2.weight\", \"vqvae.decoder.model.2.0.model.1.conv2.bias\", \"vqvae.decoder.model.2.0.model.2.conv1.weight\", \"vqvae.decoder.model.2.0.model.2.conv1.bias\", \"vqvae.decoder.model.2.0.model.2.conv2.weight\", \"vqvae.decoder.model.2.0.model.2.conv2.bias\", \"vqvae.decoder.model.2.2.weight\", \"vqvae.decoder.model.2.2.bias\", \"vqvae.decoder.model.3.0.model.0.conv1.weight\", \"vqvae.decoder.model.3.0.model.0.conv1.bias\", \"vqvae.decoder.model.3.0.model.0.conv2.weight\", \"vqvae.decoder.model.3.0.model.0.conv2.bias\", \"vqvae.decoder.model.3.0.model.1.conv1.weight\", \"vqvae.decoder.model.3.0.model.1.conv1.bias\", \"vqvae.decoder.model.3.0.model.1.conv2.weight\", \"vqvae.decoder.model.3.0.model.1.conv2.bias\", \"vqvae.decoder.model.3.0.model.2.conv1.weight\", \"vqvae.decoder.model.3.0.model.2.conv1.bias\", \"vqvae.decoder.model.3.0.model.2.conv2.weight\", \"vqvae.decoder.model.3.0.model.2.conv2.bias\", \"vqvae.decoder.model.3.2.weight\", \"vqvae.decoder.model.3.2.bias\", \"vqvae.decoder.model.4.weight\", \"vqvae.decoder.model.4.bias\", \"vqvae.decoder.model.6.weight\", \"vqvae.decoder.model.6.bias\", \"vqvae.quantizer.codebook\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m args\u001b[39m.\u001b[39mresume_pth \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/epinyoan/git/MaskText2Motion/T2M-GPT/output/VQVAE/net_last.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     32\u001b[0m ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(args\u001b[39m.\u001b[39mresume_pth, map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m net\u001b[39m.\u001b[39;49mload_state_dict(ckpt[\u001b[39m'\u001b[39;49m\u001b[39mnet\u001b[39;49m\u001b[39m'\u001b[39;49m], strict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     34\u001b[0m net\u001b[39m.\u001b[39meval()\n\u001b[1;32m     35\u001b[0m net\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/miniconda3/envs/T2M-GPT/lib/python3.8/site-packages/torch/nn/modules/module.py:1223\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1219\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1220\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1224\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1225\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VQVAETransformer:\n\tMissing key(s) in state_dict: \"encoder.sequence_embedding\", \"encoder.joint_embed.weight\", \"encoder.joint_embed.bias\", \"encoder.temporal_blocks.0.norm1.weight\", \"encoder.temporal_blocks.0.norm1.bias\", \"encoder.temporal_blocks.0.attn.qkv.weight\", \"encoder.temporal_blocks.0.attn.qkv.bias\", \"encoder.temporal_blocks.0.attn.proj.weight\", \"encoder.temporal_blocks.0.attn.proj.bias\", \"encoder.temporal_blocks.0.norm2.weight\", \"encoder.temporal_blocks.0.norm2.bias\", \"encoder.temporal_blocks.0.mlp.fc1.weight\", \"encoder.temporal_blocks.0.mlp.fc1.bias\", \"encoder.temporal_blocks.0.mlp.fc2.weight\", \"encoder.temporal_blocks.0.mlp.fc2.bias\", \"encoder.temporal_blocks.1.norm1.weight\", \"encoder.temporal_blocks.1.norm1.bias\", \"encoder.temporal_blocks.1.attn.qkv.weight\", \"encoder.temporal_blocks.1.attn.qkv.bias\", \"encoder.temporal_blocks.1.attn.proj.weight\", \"encoder.temporal_blocks.1.attn.proj.bias\", \"encoder.temporal_blocks.1.norm2.weight\", \"encoder.temporal_blocks.1.norm2.bias\", \"encoder.temporal_blocks.1.mlp.fc1.weight\", \"encoder.temporal_blocks.1.mlp.fc1.bias\", \"encoder.temporal_blocks.1.mlp.fc2.weight\", \"encoder.temporal_blocks.1.mlp.fc2.bias\", \"encoder.temporal_blocks.2.norm1.weight\", \"encoder.temporal_blocks.2.norm1.bias\", \"encoder.temporal_blocks.2.attn.qkv.weight\", \"encoder.temporal_blocks.2.attn.qkv.bias\", \"encoder.temporal_blocks.2.attn.proj.weight\", \"encoder.temporal_blocks.2.attn.proj.bias\", \"encoder.temporal_blocks.2.norm2.weight\", \"encoder.temporal_blocks.2.norm2.bias\", \"encoder.temporal_blocks.2.mlp.fc1.weight\", \"encoder.temporal_blocks.2.mlp.fc1.bias\", \"encoder.temporal_blocks.2.mlp.fc2.weight\", \"encoder.temporal_blocks.2.mlp.fc2.bias\", \"encoder.temporal_blocks.3.norm1.weight\", \"encoder.temporal_blocks.3.norm1.bias\", \"encoder.temporal_blocks.3.attn.qkv.weight\", \"encoder.temporal_blocks.3.attn.qkv.bias\", \"encoder.temporal_blocks.3.attn.proj.weight\", \"encoder.temporal_blocks.3.attn.proj.bias\", \"encoder.temporal_blocks.3.norm2.weight\", \"encoder.temporal_blocks.3.norm2.bias\", \"encoder.temporal_blocks.3.mlp.fc1.weight\", \"encoder.temporal_blocks.3.mlp.fc1.bias\", \"encoder.temporal_blocks.3.mlp.fc2.weight\", \"encoder.temporal_blocks.3.mlp.fc2.bias\", \"encoder.ln_out.weight\", \"encoder.ln_out.bias\", \"encoder.out.weight\", \"encoder.out.bias\", \"decoder.sequence_embedding\", \"decoder.joint_embed.weight\", \"decoder.joint_embed.bias\", \"decoder.temporal_blocks.0.norm1.weight\", \"decoder.temporal_blocks.0.norm1.bias\", \"decoder.temporal_blocks.0.attn.qkv.weight\", \"decoder.temporal_blocks.0.attn.qkv.bias\", \"decoder.temporal_blocks.0.attn.proj.weight\", \"decoder.temporal_blocks.0.attn.proj.bias\", \"decoder.temporal_blocks.0.norm2.weight\", \"decoder.temporal_blocks.0.norm2.bias\", \"decoder.temporal_blocks.0.mlp.fc1.weight\", \"decoder.temporal_blocks.0.mlp.fc1.bias\", \"decoder.temporal_blocks.0.mlp.fc2.weight\", \"decoder.temporal_blocks.0.mlp.fc2.bias\", \"decoder.temporal_blocks.1.norm1.weight\", \"decoder.temporal_blocks.1.norm1.bias\", \"decoder.temporal_blocks.1.attn.qkv.weight\", \"decoder.temporal_blocks.1.attn.qkv.bias\", \"decoder.temporal_blocks.1.attn.proj.weight\", \"decoder.temporal_blocks.1.attn.proj.bias\", \"decoder.temporal_blocks.1.norm2.weight\", \"decoder.temporal_blocks.1.norm2.bias\", \"decoder.temporal_blocks.1.mlp.fc1.weight\", \"decoder.temporal_blocks.1.mlp.fc1.bias\", \"decoder.temporal_blocks.1.mlp.fc2.weight\", \"decoder.temporal_blocks.1.mlp.fc2.bias\", \"decoder.temporal_blocks.2.norm1.weight\", \"decoder.temporal_blocks.2.norm1.bias\", \"decoder.temporal_blocks.2.attn.qkv.weight\", \"decoder.temporal_blocks.2.attn.qkv.bias\", \"decoder.temporal_blocks.2.attn.proj.weight\", \"decoder.temporal_blocks.2.attn.proj.bias\", \"decoder.temporal_blocks.2.norm2.weight\", \"decoder.temporal_blocks.2.norm2.bias\", \"decoder.temporal_blocks.2.mlp.fc1.weight\", \"decoder.temporal_blocks.2.mlp.fc1.bias\", \"decoder.temporal_blocks.2.mlp.fc2.weight\", \"decoder.temporal_blocks.2.mlp.fc2.bias\", \"decoder.temporal_blocks.3.norm1.weight\", \"decoder.temporal_blocks.3.norm1.bias\", \"decoder.temporal_blocks.3.attn.qkv.weight\", \"decoder.temporal_blocks.3.attn.qkv.bias\", \"decoder.temporal_blocks.3.attn.proj.weight\", \"decoder.temporal_blocks.3.attn.proj.bias\", \"decoder.temporal_blocks.3.norm2.weight\", \"decoder.temporal_blocks.3.norm2.bias\", \"decoder.temporal_blocks.3.mlp.fc1.weight\", \"decoder.temporal_blocks.3.mlp.fc1.bias\", \"decoder.temporal_blocks.3.mlp.fc2.weight\", \"decoder.temporal_blocks.3.mlp.fc2.bias\", \"decoder.ln_out.weight\", \"decoder.ln_out.bias\", \"decoder.out.weight\", \"decoder.out.bias\", \"quantizer.codebook\". \n\tUnexpected key(s) in state_dict: \"vqvae.encoder.model.0.weight\", \"vqvae.encoder.model.0.bias\", \"vqvae.encoder.model.2.0.weight\", \"vqvae.encoder.model.2.0.bias\", \"vqvae.encoder.model.2.1.model.0.conv1.weight\", \"vqvae.encoder.model.2.1.model.0.conv1.bias\", \"vqvae.encoder.model.2.1.model.0.conv2.weight\", \"vqvae.encoder.model.2.1.model.0.conv2.bias\", \"vqvae.encoder.model.2.1.model.1.conv1.weight\", \"vqvae.encoder.model.2.1.model.1.conv1.bias\", \"vqvae.encoder.model.2.1.model.1.conv2.weight\", \"vqvae.encoder.model.2.1.model.1.conv2.bias\", \"vqvae.encoder.model.2.1.model.2.conv1.weight\", \"vqvae.encoder.model.2.1.model.2.conv1.bias\", \"vqvae.encoder.model.2.1.model.2.conv2.weight\", \"vqvae.encoder.model.2.1.model.2.conv2.bias\", \"vqvae.encoder.model.3.0.weight\", \"vqvae.encoder.model.3.0.bias\", \"vqvae.encoder.model.3.1.model.0.conv1.weight\", \"vqvae.encoder.model.3.1.model.0.conv1.bias\", \"vqvae.encoder.model.3.1.model.0.conv2.weight\", \"vqvae.encoder.model.3.1.model.0.conv2.bias\", \"vqvae.encoder.model.3.1.model.1.conv1.weight\", \"vqvae.encoder.model.3.1.model.1.conv1.bias\", \"vqvae.encoder.model.3.1.model.1.conv2.weight\", \"vqvae.encoder.model.3.1.model.1.conv2.bias\", \"vqvae.encoder.model.3.1.model.2.conv1.weight\", \"vqvae.encoder.model.3.1.model.2.conv1.bias\", \"vqvae.encoder.model.3.1.model.2.conv2.weight\", \"vqvae.encoder.model.3.1.model.2.conv2.bias\", \"vqvae.encoder.model.4.weight\", \"vqvae.encoder.model.4.bias\", \"vqvae.decoder.model.0.weight\", \"vqvae.decoder.model.0.bias\", \"vqvae.decoder.model.2.0.model.0.conv1.weight\", \"vqvae.decoder.model.2.0.model.0.conv1.bias\", \"vqvae.decoder.model.2.0.model.0.conv2.weight\", \"vqvae.decoder.model.2.0.model.0.conv2.bias\", \"vqvae.decoder.model.2.0.model.1.conv1.weight\", \"vqvae.decoder.model.2.0.model.1.conv1.bias\", \"vqvae.decoder.model.2.0.model.1.conv2.weight\", \"vqvae.decoder.model.2.0.model.1.conv2.bias\", \"vqvae.decoder.model.2.0.model.2.conv1.weight\", \"vqvae.decoder.model.2.0.model.2.conv1.bias\", \"vqvae.decoder.model.2.0.model.2.conv2.weight\", \"vqvae.decoder.model.2.0.model.2.conv2.bias\", \"vqvae.decoder.model.2.2.weight\", \"vqvae.decoder.model.2.2.bias\", \"vqvae.decoder.model.3.0.model.0.conv1.weight\", \"vqvae.decoder.model.3.0.model.0.conv1.bias\", \"vqvae.decoder.model.3.0.model.0.conv2.weight\", \"vqvae.decoder.model.3.0.model.0.conv2.bias\", \"vqvae.decoder.model.3.0.model.1.conv1.weight\", \"vqvae.decoder.model.3.0.model.1.conv1.bias\", \"vqvae.decoder.model.3.0.model.1.conv2.weight\", \"vqvae.decoder.model.3.0.model.1.conv2.bias\", \"vqvae.decoder.model.3.0.model.2.conv1.weight\", \"vqvae.decoder.model.3.0.model.2.conv1.bias\", \"vqvae.decoder.model.3.0.model.2.conv2.weight\", \"vqvae.decoder.model.3.0.model.2.conv2.bias\", \"vqvae.decoder.model.3.2.weight\", \"vqvae.decoder.model.3.2.bias\", \"vqvae.decoder.model.4.weight\", \"vqvae.decoder.model.4.bias\", \"vqvae.decoder.model.6.weight\", \"vqvae.decoder.model.6.bias\", \"vqvae.quantizer.codebook\". "
     ]
    }
   ],
   "source": [
    "args.nb_code = 512 # 8192 # \n",
    "args.code_dim = 512 # 32 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.embed_dim_gpt = 1024\n",
    "args.clip_dim = 512\n",
    "args.block_size = 51\n",
    "args.num_layers = 9\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                            embed_dim=args.embed_dim_gpt, \n",
    "                                            clip_dim=args.clip_dim, \n",
    "                                            block_size=args.block_size, \n",
    "                                            num_layers=args.num_layers, \n",
    "                                            n_head=args.n_head_gpt, \n",
    "                                            drop_out_rate=args.drop_out_rate, \n",
    "                                            fc_rate=args.ff_rate)\n",
    "\n",
    "\n",
    "net = VQVAETransformer(args)\n",
    "args.resume_pth = '/home/epinyoan/git/MaskText2Motion/T2M-GPT/output/VQVAE/net_last.pth'\n",
    "# ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "# net.load_state_dict(ckpt['net'], strict=True)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()\n",
    "''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE encoder index precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VQVAETransformer' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m bs, seq \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], pose\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m pose \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39mfloat() \u001b[39m# bs, nb_joints, joints_dim, seq_len\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m target \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mencode(pose)\n\u001b[1;32m      7\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/T2M-GPT/lib/python3.8/site-packages/torch/nn/modules/module.py:947\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    946\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 947\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    948\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VQVAETransformer' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "for batch in train_loader_token:\n",
    "    pose, name = batch\n",
    "    bs, seq = pose.shape[0], pose.shape[1]\n",
    "\n",
    "    pose = pose.cuda().float() # bs, nb_joints, joints_dim, seq_len\n",
    "    target = net.encode(pose)\n",
    "    target = target.cpu().numpy()\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:01<00:00, 2952.82it/s]\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 1024\n",
    "num_workers = 8\n",
    "args.vq_name = 'VQVAE'\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, args.vq_name, unit_length=2**args.down_t, num_workers=num_workers)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## count used indices\n",
    "# all_indices = None\n",
    "# for batch in train_loader:\n",
    "#     clip_text, m_tokens, m_tokens_len = batch\n",
    "#     if all_indices is None:\n",
    "#         all_indices = m_tokens\n",
    "#     else:\n",
    "#         all_indices = torch.cat([all_indices, m_tokens])\n",
    "# a = all_indices.view(-1)\n",
    "# condition = a < 512\n",
    "# c = torch.bincount(a[condition])\n",
    "# (c==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 51, 513])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pkeep = .5\n",
    "batch = next(train_loader_iter)\n",
    "clip_text, m_tokens, m_tokens_len = batch\n",
    "m_tokens, m_tokens_len = m_tokens.cuda(), m_tokens_len.cuda()\n",
    "bs = m_tokens.shape[0]\n",
    "target = m_tokens    # (bs, 26)\n",
    "target = target.cuda()\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    \n",
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "\n",
    "input_index = target[:,:-1]\n",
    "\n",
    "if args.pkeep == -1:\n",
    "    proba = np.random.rand(1)[0]\n",
    "    mask = torch.bernoulli(proba * torch.ones(input_index.shape,\n",
    "                                                        device=input_index.device))\n",
    "else:\n",
    "    mask = torch.bernoulli(args.pkeep * torch.ones(input_index.shape,\n",
    "                                                        device=input_index.device))\n",
    "mask = mask.round().to(dtype=torch.int64)\n",
    "r_indices = torch.randint_like(input_index, args.nb_code)\n",
    "a_indices = mask*input_index+(1-mask)*r_indices\n",
    "\n",
    "cls_pred = trans_encoder(a_indices, feat_clip_text)\n",
    "cls_pred = cls_pred.contiguous()\n",
    "cls_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.if_maxtest = False\n",
    "\n",
    "right_num = 0\n",
    "loss_ce = torch.nn.CrossEntropyLoss()\n",
    "loss_cls = 0.0\n",
    "for i in range(bs):\n",
    "    # loss function     (26), (26, 513)\n",
    "    loss_cls += loss_ce(cls_pred[i][:m_tokens_len[i] + 1], target[i][:m_tokens_len[i] + 1]) / bs\n",
    "\n",
    "    # Accuracy\n",
    "    probs = torch.softmax(cls_pred[i][:m_tokens_len[i] + 1], dim=-1)\n",
    "\n",
    "    if args.if_maxtest:\n",
    "        _, cls_pred_index = torch.max(probs, dim=-1)\n",
    "    else:\n",
    "        dist = Categorical(probs)\n",
    "        cls_pred_index = dist.sample()\n",
    "    right_num += (cls_pred_index.flatten(0) == target[i][:m_tokens_len[i] + 1].flatten(0)).sum().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 3027.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume_trans = 'output/1_TRANS/net_best_fid.pth'\n",
    "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "trans_encoder.load_state_dict(ckpt['trans'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run only one batch ###\n",
    "batch = next(iter(val_loader))\n",
    "word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token, name = batch\n",
    "bs, seq = pose.shape[:2]\n",
    "num_joints = 21 if pose.shape[-1] == 251 else 22\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "\n",
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "pred_pose_eval = torch.zeros((bs, seq, pose.shape[-1])).cuda()\n",
    "pred_len = torch.ones(bs).long()\n",
    "for k in range(bs):\n",
    "    index_motion = trans_encoder.sample(feat_clip_text[k:k+1], False)\n",
    "    pred_pose = net.forward_decoder(index_motion)\n",
    "    cur_len = pred_pose.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run full eval (slow) ###\n",
    "args.out_dir = './'\n",
    "best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = \\\n",
    "    eval_trans.evaluation_transformer(args.out_dir, \n",
    "        val_loader, \n",
    "        net, \n",
    "        trans_encoder, \n",
    "        logger, \n",
    "        writer, \n",
    "        0, \n",
    "        best_fid=1000, \n",
    "        best_iter=0, \n",
    "        best_div=100, \n",
    "        best_top1=0, \n",
    "        best_top2=0, \n",
    "        best_top3=0, \n",
    "        best_matching=100, \n",
    "        clip_model=clip_model, \n",
    "        eval_wrapper=eval_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
