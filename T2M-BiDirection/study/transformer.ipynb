{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.VQVAE_transformer import VQVAETransformer\n",
    "from exit.motiontransformer import MotionTransformerOnly2, generate_src_mask\n",
    "from exit.utils import get_model, visualize_2motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:01<00:00, 3237.77it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 2743.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Reading checkpoints/kit/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 30) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 'kit'\n",
    "args.down_t = 2\n",
    "\n",
    "train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, num_workers=1, unit_length=2**args.down_t)\n",
    "\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)\n",
    "\n",
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nb_code = 512 # 8192 # \n",
    "args.code_dim = 128 #512 # 32 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.embed_dim_gpt = 1024\n",
    "args.clip_dim = 512\n",
    "args.block_size = 51\n",
    "args.num_layers = 9\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                            embed_dim=args.embed_dim_gpt, \n",
    "                                            clip_dim=args.clip_dim, \n",
    "                                            block_size=args.block_size, \n",
    "                                            num_layers=args.num_layers, \n",
    "                                            n_head=args.n_head_gpt, \n",
    "                                            drop_out_rate=args.drop_out_rate, \n",
    "                                            fc_rate=args.ff_rate)\n",
    "\n",
    "\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exit.VQVAE_transformer import VQVAETransformer\n",
    "net = VQVAETransformer(args).cuda()\n",
    "net = torch.nn.DataParallel(net)\n",
    "args.resume_pth = '/home/epinyoan/git/MaskText2Motion/T2M-BiDirection/output/VQT_4_4ly_emb256_cbEmb128_fixL1_noZero2/net_last.pth'\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE encoder index precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader_token:\n",
    "    pose, name = batch\n",
    "    bs, seq = pose.shape[0], pose.shape[1]\n",
    "\n",
    "    pose = pose.cuda().float() # bs, nb_joints, joints_dim, seq_len\n",
    "    # target = get_model(net).encode(pose)\n",
    "    # target = target.cpu().numpy()\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:01<00:00, 2952.82it/s]\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 1024\n",
    "num_workers = 8\n",
    "args.vq_name = 'VQVAE'\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, args.vq_name, unit_length=2**args.down_t, num_workers=num_workers)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## count used indices\n",
    "# all_indices = None\n",
    "# for batch in train_loader:\n",
    "#     clip_text, m_tokens, m_tokens_len = batch\n",
    "#     if all_indices is None:\n",
    "#         all_indices = m_tokens\n",
    "#     else:\n",
    "#         all_indices = torch.cat([all_indices, m_tokens])\n",
    "# a = all_indices.view(-1)\n",
    "# condition = a < 512\n",
    "# c = torch.bincount(a[condition])\n",
    "# (c==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 51, 513])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pkeep = .5\n",
    "batch = next(train_loader_iter)\n",
    "clip_text, m_tokens, m_tokens_len = batch\n",
    "m_tokens, m_tokens_len = m_tokens.cuda(), m_tokens_len.cuda()\n",
    "bs = m_tokens.shape[0]\n",
    "target = m_tokens    # (bs, 26)\n",
    "target = target.cuda()\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    \n",
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "\n",
    "input_index = target[:,:-1]\n",
    "\n",
    "if args.pkeep == -1:\n",
    "    proba = np.random.rand(1)[0]\n",
    "    mask = torch.bernoulli(proba * torch.ones(input_index.shape,\n",
    "                                                        device=input_index.device))\n",
    "else:\n",
    "    mask = torch.bernoulli(args.pkeep * torch.ones(input_index.shape,\n",
    "                                                        device=input_index.device))\n",
    "mask = mask.round().to(dtype=torch.int64)\n",
    "r_indices = torch.randint_like(input_index, args.nb_code)\n",
    "a_indices = mask*input_index+(1-mask)*r_indices\n",
    "\n",
    "cls_pred = trans_encoder(a_indices, feat_clip_text)\n",
    "cls_pred = cls_pred.contiguous()\n",
    "cls_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.if_maxtest = False\n",
    "\n",
    "right_num = 0\n",
    "loss_ce = torch.nn.CrossEntropyLoss()\n",
    "loss_cls = 0.0\n",
    "for i in range(bs):\n",
    "    # loss function     (26), (26, 513)\n",
    "    loss_cls += loss_ce(cls_pred[i][:m_tokens_len[i] + 1], target[i][:m_tokens_len[i] + 1]) / bs\n",
    "\n",
    "    # Accuracy\n",
    "    probs = torch.softmax(cls_pred[i][:m_tokens_len[i] + 1], dim=-1)\n",
    "\n",
    "    if args.if_maxtest:\n",
    "        _, cls_pred_index = torch.max(probs, dim=-1)\n",
    "    else:\n",
    "        dist = Categorical(probs)\n",
    "        cls_pred_index = dist.sample()\n",
    "    right_num += (cls_pred_index.flatten(0) == target[i][:m_tokens_len[i] + 1].flatten(0)).sum().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 3027.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume_trans = 'output/1_TRANS/net_best_fid.pth'\n",
    "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "trans_encoder.load_state_dict(ckpt['trans'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run only one batch ###\n",
    "batch = next(iter(val_loader))\n",
    "word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token, name = batch\n",
    "bs, seq = pose.shape[:2]\n",
    "num_joints = 21 if pose.shape[-1] == 251 else 22\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "\n",
    "feat_clip_text = clip_model.encode_text(text).float()\n",
    "pred_pose_eval = torch.zeros((bs, seq, pose.shape[-1])).cuda()\n",
    "pred_len = torch.ones(bs).long()\n",
    "for k in range(bs):\n",
    "    index_motion = trans_encoder.sample(feat_clip_text[k:k+1], False)\n",
    "    pred_pose = net.forward_decoder(index_motion)\n",
    "    cur_len = pred_pose.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run full eval (slow) ###\n",
    "args.out_dir = './'\n",
    "best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = \\\n",
    "    eval_trans.evaluation_transformer(args.out_dir, \n",
    "        val_loader, \n",
    "        net, \n",
    "        trans_encoder, \n",
    "        logger, \n",
    "        writer, \n",
    "        0, \n",
    "        best_fid=1000, \n",
    "        best_iter=0, \n",
    "        best_div=100, \n",
    "        best_top1=0, \n",
    "        best_top2=0, \n",
    "        best_top3=0, \n",
    "        best_matching=100, \n",
    "        clip_model=clip_model, \n",
    "        eval_wrapper=eval_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
