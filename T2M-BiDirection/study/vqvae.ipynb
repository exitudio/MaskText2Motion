{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import models.vqvae as vqvae\n",
    "import utils.losses as losses \n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_VQ, dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from exit.VQVAE_transformer import VQVAETransformer\n",
    "from exit.motiontransformer import MotionTransformerOnly2, generate_src_mask\n",
    "from exit.utils import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n",
      "Reading ./checkpoints/kit/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 30) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 'kit'\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "\n",
    "if args.dataname == 'kit' : \n",
    "    dataset_opt_path = './checkpoints/kit/Comp_v6_KLD005/opt.txt'  \n",
    "    args.nb_joints = 21\n",
    "else :\n",
    "    dataset_opt_path = './checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "    args.nb_joints = 22\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:01<00:00, 3674.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 8 #256\n",
    "args.down_t = 2\n",
    "args.vqvae_transformer = True \n",
    "args.window_size = 64\n",
    "args.window_size = args.window_size if not args.vqvae_transformer else -1\n",
    "train_loader = dataset_VQ.DATALoader(args.dataname,\n",
    "                                        args.batch_size,\n",
    "                                        window_size=args.window_size,\n",
    "                                        num_workers = 1,\n",
    "                                        unit_length=2**args.down_t)\n",
    "train_loader_iter = dataset_VQ.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nb_code = 512 # 8192 # \n",
    "args.code_dim = 128 #512 # 32 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.recons_loss = 'l1_smooth'\n",
    "\n",
    "if args.vqvae_transformer:\n",
    "    net = VQVAETransformer(args).cuda()\n",
    "    Loss = losses.ReConsLoss(args.recons_loss, args.nb_joints, True)\n",
    "else:\n",
    "    net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                        args.nb_code,\n",
    "                        args.code_dim,\n",
    "                        args.output_emb_width,\n",
    "                        args.down_t,\n",
    "                        args.stride_t,\n",
    "                        args.width,\n",
    "                        args.depth,\n",
    "                        args.dilation_growth_rate,\n",
    "                        args.vq_act,\n",
    "                        args.vq_norm)\n",
    "    Loss = losses.ReConsLoss(args.recons_loss, args.nb_joints)\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "args.resume_pth = '/home/epinyoan/git/MaskText2Motion/T2M-BiDirection/output/VQT_4_4ly_emb256_cbEmb128_fixL1_noZero2/net_last.pth'\n",
    "# ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "# net.load_state_dict(ckpt['net'], strict=True)\n",
    "\n",
    "net.train()\n",
    "net.cuda()\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get_model(net).quantizer.codebook = torch.ones(args.nb_code, args.code_dim).cuda()\n",
    "# get_model(net).quantizer.codebook\n",
    "# # ckpt['net']['module.quantizer.codebook']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.eval()\n",
    "# for gt_motion, m_length in train_loader_iter:\n",
    "#     gt_motion = gt_motion.cuda().float() # (bs, 64, dim)\n",
    "#     src_mask = generate_src_mask(gt_motion.shape[1], m_length).to(gt_motion.device).unsqueeze(-1)\n",
    "#     gt_motion = gt_motion * src_mask\n",
    "#     code_idx = get_model(net).encode(gt_motion, src_mask)\n",
    "    \n",
    "#     x_recon, perplexity, z, z_q = get_model(net)(gt_motion, src_mask)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_idx.view(8, 196)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = gt_motion[0].detach().cpu().numpy()\n",
    "# y = x_recon[0].detach().cpu().numpy()\n",
    "# visualize_2motions(x, train_loader.dataset.std, train_loader.dataset.mean, args.dataname, m_length[0], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.code_sum: cuda:0 cuda:0\n",
      "self.code_sum: cuda:1 cuda:1\n",
      "self.code_sum: cuda:2 cuda:2\n",
      "self.code_sum: cuda:3 cuda:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.3373e-01,  4.2515e-01,  3.7443e-02,  ..., -4.1717e-01,\n",
       "          5.6350e-02,  4.9207e-01],\n",
       "        [ 5.2048e-01, -3.9404e-01, -2.7206e-01,  ...,  5.6137e-01,\n",
       "          7.2625e-01,  4.0026e-01],\n",
       "        [ 1.7480e-01, -2.9250e-01,  2.6810e-01,  ...,  1.7266e-01,\n",
       "          3.0669e-01, -8.7324e-01],\n",
       "        ...,\n",
       "        [-4.4521e-04, -5.5708e-04, -3.6564e-04,  ..., -3.9043e-04,\n",
       "         -4.7239e-04, -1.8714e-03],\n",
       "        [-6.8156e-04, -6.0669e-04, -1.1575e-04,  ...,  2.5260e-04,\n",
       "          4.1283e-04, -7.8461e-04],\n",
       "        [ 1.2244e-04, -2.3654e-04, -3.4986e-04,  ..., -3.2664e-04,\n",
       "         -9.9283e-05, -1.0826e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.commit = 0.02\n",
    "args.loss_vel = 0.5\n",
    "\n",
    "gt_motion, m_length = next(train_loader_iter)\n",
    "gt_motion = gt_motion.cuda().float() # (bs, 64, dim)\n",
    "net.train()\n",
    "if args.vqvae_transformer:\n",
    "    src_mask = generate_src_mask(gt_motion.shape[1], m_length).to(gt_motion.device).unsqueeze(-1)\n",
    "    gt_motion = gt_motion * src_mask\n",
    "    pred_motion, perplexity, z, z_q = net(gt_motion, src_mask)\n",
    "    perplexity = perplexity.mean()\n",
    "    ## lifted from quantize_cnn to calculate perplexity across all batch\n",
    "    # code_count = code_onehot.sum(dim=-1)\n",
    "    # prob = code_count / torch.sum(code_count)  \n",
    "    # perplexity = torch.exp(-torch.sum(prob * torch.log(prob + 1e-7)))\n",
    "\n",
    "    loss_commit = (z**2 - z_q.detach()**2).sum() / (src_mask.sum() * z.shape[-1])\n",
    "    loss_vel = Loss.forward_vel(pred_motion, gt_motion, src_mask)\n",
    "    loss_motion = Loss(pred_motion, gt_motion, src_mask)\n",
    "else:\n",
    "    pred_motion, loss_commit, perplexity = net(gt_motion)\n",
    "    loss_vel = Loss.forward_vel(pred_motion, gt_motion)\n",
    "    loss_motion = Loss(pred_motion, gt_motion)\n",
    "\n",
    "loss = loss_motion + args.commit * loss_commit + args.loss_vel * loss_vel\n",
    "get_model(net).quantizer.codebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 3045.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False,\n",
    "                                        32,\n",
    "                                        w_vectorizer,\n",
    "                                        unit_length=2**args.down_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 1 :, FID. 99.1333, Diversity Real. 11.0824, Diversity. 0.7526, R_precision_real. [0.44097222 0.67013889 0.77777778], R_precision. [0.02430556 0.04513889 0.06597222], matching_score_real. 2.7021690474616156, matching_score_pred. 10.166575961642796\n",
      "./Test/FID 99.13334489606402 1\n",
      "./Test/Diversity 0.752582 1\n",
      "./Test/top1 0.024305555555555556 1\n",
      "./Test/top2 0.04513888888888889 1\n",
      "./Test/top3 0.06597222222222222 1\n",
      "./Test/matching_score 10.166575961642796 1\n"
     ]
    }
   ],
   "source": [
    "args.out_dir = './'\n",
    "a = eval_trans.evaluation_vqvae(\n",
    "    args.out_dir, \n",
    "    val_loader, \n",
    "    net, \n",
    "    logger, \n",
    "    writer, \n",
    "    nb_iter = 1, \n",
    "    best_fid = 1, \n",
    "    best_iter = 1, \n",
    "    best_div = 1, \n",
    "    best_top1 = 1, \n",
    "    best_top2 = 1, \n",
    "    best_top3 = 1, \n",
    "    best_matching = 1, \n",
    "    eval_wrapper=eval_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 196, 251])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = next(iter(val_loader))\n",
    "motion.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoseFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exit.motiontransformer import MotionTransformerOnly2, generate_src_mask\n",
    "encoder = MotionTransformerOnly2(input_feats=251, \n",
    "                                            output_feats=32, \n",
    "                                            latent_dim=256, \n",
    "                                            num_layers=8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 196, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motions = torch.rand(2, 196, 251).cuda()\n",
    "B, N, _ = motions.shape\n",
    "num_heads = 8\n",
    "length = torch.tensor([50, 80]).cuda()\n",
    "src_mask = generate_src_mask(motions.shape[1], length).to(motions.device).unsqueeze(-1)\n",
    "src_mask_attn = src_mask.view(B, 1, 1, N).repeat(1, num_heads, N, 1)\n",
    "encoder(motions, src_mask=src_mask_attn).shape\n",
    "src_mask.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
