{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask, uniform, cosine_schedule\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/111\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        return self.model.encode_text(text)\n",
    "clip_model = TextCLIP(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:06<00:00, 3463.84it/s]\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = 't2m'\n",
    "args.nb_code = 8192 # 512 # \n",
    "args.code_dim = 32 # 512 # \n",
    "args.batch_size = 512\n",
    "args.down_t = 2\n",
    "num_workers = 8\n",
    "codebook_dir = '/home/epinyoan/git/MaskText2Motion/T2M-BD/output/vq/2023-07-19-04-17-17_12_VQVAE_20batchResetNRandom_8192_32/codebook'\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t, num_workers=num_workers)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def init_weight(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose1d):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        # m.bias.data.fill_(0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class LengthPredictorCLIP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        nd = 512\n",
    "        dropout_p = 0.1\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(input_size, nd),\n",
    "            nn.LayerNorm(nd),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(nd, nd // 2),\n",
    "            nn.LayerNorm(nd // 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(nd // 2, nd // 4),\n",
    "            nn.LayerNorm(nd // 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(nd // 4, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, word_embs):\n",
    "        return self.output(word_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_norm(network_list):\n",
    "    for network in network_list:\n",
    "        torch.nn.utils.clip_grad_norm_(network.parameters(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:02<00:00, 2010.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, True, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_testset():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token, name in tqdm(val_loader):\n",
    "        text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "        feat_clip_text = clip_model(text).float()\n",
    "        pred_prob_len = len_predictor(feat_clip_text)\n",
    "        pred_prob_len = pred_prob_len.argsort(dim=-1, descending=True)[:, 0]\n",
    "        correct += torch.isclose(pred_prob_len*4, m_length.cuda(), atol= 4).sum()\n",
    "        total += pred_prob_len.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./output/length_predictor/TEMP')\n",
    "len_predictor = LengthPredictorCLIP(512, 50)\n",
    "# len_predictor = torch.nn.DataParallel(len_predictor)\n",
    "len_predictor.cuda()\n",
    "crossEntropy = torch.nn.CrossEntropyLoss()\n",
    "softmax = torch.nn.Softmax(-1)\n",
    "optimizer = torch.optim.Adam(len_predictor.parameters(), lr=1e-4)\n",
    "\n",
    "__log_acc_epoch = 20\n",
    "\n",
    "num_iter = len(train_loader)\n",
    "for epoch in tqdm(range(300)):\n",
    "    avg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for clip_text, target, m_tokens_len in train_loader:\n",
    "        text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "        feat_clip_text = clip_model(text).float()\n",
    "        pred_prob_len = len_predictor(feat_clip_text)\n",
    "        # pred_prob_len = softmax(pred_prob_len)\n",
    "        \n",
    "        noise = (torch.rand(m_tokens_len.shape[0]) * 3).int() - 1\n",
    "        m_tokens_len = (m_tokens_len + noise).clamp(max=49)\n",
    "        loss = crossEntropy(pred_prob_len, m_tokens_len.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(len_predictor.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        \n",
    "        avg_loss += loss/num_iter\n",
    "        \n",
    "        if epoch % __log_acc_epoch == 0:\n",
    "            pred_tok_len = pred_prob_len.argsort(dim=-1, descending=True)[:, 0]\n",
    "            correct += torch.isclose(pred_tok_len, m_tokens_len.cuda(), atol= 1).sum()\n",
    "            total += pred_tok_len.shape[0]\n",
    "\n",
    "    writer.add_scalar('./Loss/all', avg_loss, epoch)\n",
    "    if epoch % __log_acc_epoch == 0:\n",
    "        writer.add_scalar('./acc', correct/total, epoch)\n",
    "        writer.add_scalar('./acc_test', eval_testset(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
