{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" #,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask, uniform, cosine_schedule\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/111\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        return self.model.encode_text(text)\n",
    "clip_model = TextCLIP(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:05<00:00, 4105.32it/s]\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = 't2m'\n",
    "args.nb_code = 8192 # 512 # \n",
    "args.code_dim = 32 # 512 # \n",
    "args.batch_size = 512\n",
    "args.down_t = 2\n",
    "num_workers = 8\n",
    "codebook_dir = '/home/epinyoan/git/MaskText2Motion/T2M-BD/output/vq/2023-07-19-04-17-17_12_VQVAE_20batchResetNRandom_8192_32/codebook'\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t, num_workers=num_workers)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def init_weight(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose1d):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        # m.bias.data.fill_(0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class LengthPredictorCLIP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        nd = 512\n",
    "        dropout_p = 0.1\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(input_size + 1, nd),\n",
    "            nn.LayerNorm(nd),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(nd, nd // 2),\n",
    "            nn.LayerNorm(nd // 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(nd // 2, nd // 4),\n",
    "            nn.LayerNorm(nd // 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(p=dropout_p, inplace=False),\n",
    "            nn.Linear(nd // 4, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, word_embs):\n",
    "        return self.output(word_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_norm(network_list):\n",
    "    for network in network_list:\n",
    "        torch.nn.utils.clip_grad_norm_(network.parameters(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:01<00:00, 2581.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    }
   ],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, True, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_testset():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token, name in tqdm(val_loader):\n",
    "        text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "        feat_clip_text = clip_model(text).float()\n",
    "        pred_prob_len = len_predictor(feat_clip_text)\n",
    "        pred_prob_len = pred_prob_len.argsort(dim=-1, descending=True)[:, 0]\n",
    "        correct += torch.isclose(pred_prob_len*4, m_length.cuda(), atol= 4).sum()\n",
    "        total += pred_prob_len.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:39<1:38:35, 19.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m text_len \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(text_len)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m text \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(clip_text, truncate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m feat_clip_text \u001b[39m=\u001b[39m clip_model(text)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m feat_clip_text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat( [feat_clip_text, text_len[:, \u001b[39mNone\u001b[39;00m]], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m pred_prob_len \u001b[39m=\u001b[39m len_predictor(feat_clip_text)\n",
      "File \u001b[0;32m~/miniconda3/envs/T2M-GPT/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,text):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcci-wins-heavy2/home/epinyoan/git/MaskText2Motion/T2M-BD/study/len_predictor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode_text(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/T2M-GPT/lib/python3.8/site-packages/clip/model.py:354\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    350\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    352\u001b[0m \u001b[39m# x.shape = [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m x \u001b[39m=\u001b[39m x[torch\u001b[39m.\u001b[39;49marange(x\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]), text\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)] \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_projection\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./output/length_predictor/TEMP')\n",
    "len_predictor = LengthPredictorCLIP(512, 50)\n",
    "# len_predictor = torch.nn.DataParallel(len_predictor)\n",
    "len_predictor.cuda()\n",
    "crossEntropy = torch.nn.CrossEntropyLoss()\n",
    "softmax = torch.nn.Softmax(-1)\n",
    "optimizer = torch.optim.Adam(len_predictor.parameters(), lr=1e-4)\n",
    "\n",
    "__log_acc_epoch = 20\n",
    "\n",
    "num_iter = len(train_loader)\n",
    "for epoch in tqdm(range(300)):\n",
    "    avg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for clip_text, target, m_tokens_len in train_loader:\n",
    "        text_len = []\n",
    "        for t in clip_text:\n",
    "            text_len.append(len(t[0].split(' ')))\n",
    "        text_len = torch.tensor(text_len).cuda()\n",
    "        \n",
    "        text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "        feat_clip_text = clip_model(text).float()\n",
    "        feat_clip_text = torch.cat( [feat_clip_text, text_len[:, None]], axis=-1)\n",
    "        pred_prob_len = len_predictor(feat_clip_text)\n",
    "        # pred_prob_len = softmax(pred_prob_len)\n",
    "        \n",
    "        noise = (torch.rand(m_tokens_len.shape[0]) * 3).int() - 1\n",
    "        m_tokens_len = (m_tokens_len + noise).clamp(max=49)\n",
    "        loss = crossEntropy(pred_prob_len, m_tokens_len.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(len_predictor.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        \n",
    "        avg_loss += loss/num_iter\n",
    "        \n",
    "        if epoch % __log_acc_epoch == 0:\n",
    "            pred_tok_len = pred_prob_len.argsort(dim=-1, descending=True)[:, 0]\n",
    "            correct += torch.isclose(pred_tok_len, m_tokens_len.cuda(), atol= 1).sum()\n",
    "            total += pred_tok_len.shape[0]\n",
    "\n",
    "    # writer.add_scalar('./Loss/all', avg_loss, epoch)\n",
    "    # if epoch % __log_acc_epoch == 0:\n",
    "    #     writer.add_scalar('./acc', correct/total, epoch)\n",
    "    #     writer.add_scalar('./acc_test', eval_testset(), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0676, -0.2576,  0.2432,  ...,  0.0280, -0.0542,  1.0000],\n",
       "        [ 0.1818, -0.2228,  0.0853,  ..., -0.0578,  0.1316,  1.0000],\n",
       "        [ 0.2673,  0.0240, -0.1853,  ...,  0.0281,  0.1201,  1.0000],\n",
       "        ...,\n",
       "        [-0.0173, -0.3701, -0.2949,  ...,  0.3525,  0.1028,  1.0000],\n",
       "        [-0.0513, -0.1121, -0.2092,  ...,  0.0769, -0.2573,  1.0000],\n",
       "        [ 0.1356,  0.1646, -0.0118,  ...,  0.2089, -0.1202,  1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
