{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" #,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans_uplow as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask, uniform, cosine_schedule\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1460 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 2759.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 't2m'\n",
    "args.down_t = 2\n",
    "\n",
    "# train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)\n",
    "\n",
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/111\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        return self.model.encode_text(text)\n",
    "clip_model = TextCLIP(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nb_code = 8192 # 512 # \n",
    "args.code_dim = 32 # 512 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.clip_dim = 512\n",
    "args.block_size = 51\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "\n",
    "\n",
    "######## LARGE ########\n",
    "args.num_layers = 9\n",
    "args.embed_dim_gpt = 1024\n",
    "args.resume_trans = '/home/epinyoan/git/MaskText2Motion/T2M-BD/output/t2m/2023-08-04-23-08-30_HML3D_36_token1stStage_cdim8192_32_lr0.0001_mask.5-1/net_last.pth'\n",
    "args.resume_pth = f'/home/epinyoan/git/MaskText2Motion/T2M-BD/output/vq/2023-08-08-00-29-40_16_VQVAE_upperlower_notShareCB_20batchResetNRandom_8192x32/net_last.pth'\n",
    "\n",
    "if args.dataset_name == 'kit':\n",
    "    args.vq_name = 'VQVAE'\n",
    "elif args.dataset_name == 't2m':\n",
    "    args.vq_name = '2023-08-08-00-29-40_16_VQVAE_upperlower_notShareCB_20batchResetNRandom_8192x32'\n",
    "\n",
    "\n",
    "args.exp_name = 'TEMP'\n",
    "args.out_dir = 'output_GPT_Final'\n",
    "args.out_dir = os.path.join(args.out_dir, f'{args.exp_name}')\n",
    "args.vq_dir= f'output/vq/{args.vq_name}'\n",
    "codebook_dir = f'{args.vq_dir}/codebook/'\n",
    "os.makedirs(args.out_dir, exist_ok = True)\n",
    "os.makedirs(args.vq_dir, exist_ok = True)\n",
    "os.makedirs(codebook_dir, exist_ok = True)\n",
    "\n",
    "from models.vqvae_sep import VQVAE_SEP\n",
    "net = VQVAE_SEP(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate)\n",
    "class VQVAE_WRAPPER(torch.nn.Module):\n",
    "    def __init__(self, vqvae) :\n",
    "        super().__init__()\n",
    "        self.vqvae = vqvae\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.vqvae(*args, **kwargs)\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "net = VQVAE_WRAPPER(net)\n",
    "    \n",
    "trans_encoder = trans.Text2Motion_Transformer(net, \n",
    "                                    num_vq=args.nb_code, \n",
    "                                    embed_dim=args.embed_dim_gpt, \n",
    "                                    clip_dim=args.clip_dim, \n",
    "                                    block_size=args.block_size, \n",
    "                                    num_layers=args.num_layers, \n",
    "                                    n_head=args.n_head_gpt, \n",
    "                                    drop_out_rate=args.drop_out_rate, \n",
    "                                    fc_rate=args.ff_rate)\n",
    "\n",
    "\n",
    "\n",
    "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "# trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder = torch.nn.DataParallel(trans_encoder)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "if len(os.listdir(codebook_dir)) == 0:\n",
    "    train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "    for batch in tqdm(train_loader_token):\n",
    "        pose, name = batch\n",
    "        bs, seq = pose.shape[0], pose.shape[1]\n",
    "\n",
    "        pose = pose.cuda().float() # bs, nb_joints, joints_dim, seq_len\n",
    "        target = net(pose, type='encode')\n",
    "        target = target.cpu().numpy()\n",
    "        np.save(pjoin(codebook_dir, name[0] +'.npy'), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:06<00:00, 3550.26it/s]\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 2\n",
    "num_workers = 8\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t, num_workers=num_workers, up_low_sep=True)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pkeep = .5\n",
    "batch = next(iter(train_loader))\n",
    "clip_text, target, m_tokens_len = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = target.shape[:2]\n",
    "target, m_tokens_len = target.cuda(), m_tokens_len.cuda()\n",
    "bs = target.shape[0]\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "\n",
    "feat_clip_text = clip_model(text).float()\n",
    "target_upper = target[..., 0]\n",
    "target_lower = target[..., 1]\n",
    "\n",
    "# [INFO] Swap input index\n",
    "if args.pkeep == -1:\n",
    "    proba = np.random.rand(1)[0]\n",
    "    mask = torch.bernoulli(proba * torch.ones(target_upper.shape,\n",
    "                                                        device=target.device))\n",
    "else:\n",
    "    mask = torch.bernoulli(args.pkeep * torch.ones(target_upper.shape,\n",
    "                                                        device=target.device))\n",
    "# random only motion token (not pad token). To prevent pad token got mixed up.\n",
    "seq_mask_no_end = generate_src_mask(max_len, m_tokens_len)\n",
    "mask = torch.logical_or(mask, ~seq_mask_no_end).int()\n",
    "r_indices = torch.randint_like(target_upper, args.nb_code)\n",
    "input_indices = mask*target_upper+(1-mask)*r_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "proba = random.randint(0, 2)/10\n",
    "mask_lower = torch.bernoulli(proba * torch.ones(target_lower.shape,\n",
    "                                                        device=target.device))\n",
    "mask_lower = torch.logical_or(mask_lower, ~seq_mask_no_end).int()\n",
    "r_indices_lower = torch.randint_like(target_lower, args.nb_code)\n",
    "input_indices_lower = mask_lower*target_upper+(1-mask_lower)*r_indices_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time step masking\n",
    "mask_id = get_model(net).vqvae.num_code + 2\n",
    "# rand_time = uniform((batch_size,), device = target.device)\n",
    "# rand_mask_probs = cosine_schedule(rand_time)\n",
    "rand_mask_probs = torch.zeros(batch_size, device = m_tokens_len.device).float().uniform_(0.5, 1)\n",
    "num_token_masked = (m_tokens_len * rand_mask_probs).round().clamp(min = 1)\n",
    "seq_mask = generate_src_mask(max_len, m_tokens_len+1)\n",
    "batch_randperm = torch.rand((batch_size, max_len), device = target.device) - seq_mask_no_end.int()\n",
    "batch_randperm = batch_randperm.argsort(dim = -1)\n",
    "mask_token = batch_randperm < rearrange(num_token_masked, 'b -> b 1')\n",
    "\n",
    "# masked_target = torch.where(mask_token, input=input_indices, other=-1)\n",
    "masked_input_indices = torch.where(mask_token, mask_id, input_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_txt = None # CFG: torch.rand((seq_mask.shape[0], 1)) > 0.1\n",
    "cls_pred = trans_encoder(masked_input_indices, input_indices_lower, feat_clip_text, src_mask = seq_mask, att_txt=att_txt)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [INFO] Compute xent loss as a batch\n",
    "weights = seq_mask_no_end / (seq_mask_no_end.sum(-1).unsqueeze(-1) * seq_mask_no_end.shape[0])\n",
    "cls_pred_seq_masked = cls_pred[seq_mask_no_end, :].view(-1, cls_pred.shape[-1])\n",
    "target_seq_masked = target_upper[seq_mask_no_end]\n",
    "weight_seq_masked = weights[seq_mask_no_end]\n",
    "loss_cls = F.cross_entropy(cls_pred_seq_masked, target_seq_masked, reduction = 'none')\n",
    "loss_cls = (loss_cls * weight_seq_masked).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 201/4384 [00:00<00:02, 2000.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:02<00:00, 1938.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, True, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4808988790027797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Test Single Sample ##########\n",
    "# word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token, name = next(iter(val_loader))\n",
    "# seq = pose.shape[1]\n",
    "# m_length = m_length.cuda()\n",
    "m_length = torch.tensor([180, 54]).cuda() # length: [196, 54]\n",
    "m_tokens_len = torch.ceil((m_length)/4)\n",
    "feat_clip_text = torch.rand([2, 512]) # [2, 512]\n",
    "feat_clip_text = feat_clip_text.cuda()\n",
    "\n",
    "target_lower = torch.randint(0, 8192, [2, 50])\n",
    "seq_mask_no_end = generate_src_mask(max_len, m_tokens_len)\n",
    "target_lower[~seq_mask_no_end] = 8193\n",
    "target_lower = target_lower.cuda()\n",
    "target_lower.scatter_(dim=-1, index=m_tokens_len[:, None].long(), value=8192)\n",
    "\n",
    "import timeit\n",
    "total = 0\n",
    "num_all = 0\n",
    "blank_id = get_model(trans_encoder).num_vq\n",
    "for k in range(1):\n",
    "    start = timeit.default_timer()\n",
    "    index_motion, temp = trans_encoder(feat_clip_text, target_lower, type=\"sample\", m_length=m_length, if_test=True, CFG=-1)\n",
    "    index_motion = torch.cat((index_motion[..., None], target_lower[..., None]), dim=-1)\n",
    "    pred_pose = net(index_motion[k:k+1, :int(m_tokens_len[k].item())], type='decode')\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    total += (stop - start)\n",
    "    num_all += 1\n",
    "total/num_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose_eval, pose, m_length, clip_text, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, best_multi, writer, logger = \\\n",
    "    eval_trans.evaluation_transformer_uplow(args.out_dir, \n",
    "        val_loader, \n",
    "        net, \n",
    "        trans_encoder, \n",
    "        logger, \n",
    "        writer, \n",
    "        0, \n",
    "        best_fid=1000, \n",
    "        best_iter=0, \n",
    "        best_div=100, \n",
    "        best_top1=0, \n",
    "        best_top2=0, \n",
    "        best_top3=0, \n",
    "        best_matching=100, \n",
    "        clip_model=clip_model, \n",
    "        dataname=args.dataname,\n",
    "        eval_wrapper=eval_wrapper,\n",
    "        num_repeat=1,\n",
    "        rand_pos=True,\n",
    "        CFG=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
