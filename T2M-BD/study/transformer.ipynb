{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask, uniform, cosine_schedule\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:02<00:00, 2110.97it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1800.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Reading checkpoints/kit/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 30) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 'kit'\n",
    "args.down_t = 2\n",
    "\n",
    "train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)\n",
    "\n",
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/111\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        return self.model.encode_text(text)\n",
    "clip_model = TextCLIP(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nb_code = 512 # 8192 # \n",
    "args.code_dim = 512 # 32 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.embed_dim_gpt = 1024\n",
    "args.clip_dim = 512\n",
    "args.block_size = 51+1\n",
    "args.num_layers = 9\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "\n",
    "args.vq_name = 'VQVAE'\n",
    "args.exp_name = 'TEMP'\n",
    "args.out_dir = 'output_GPT_Final'\n",
    "args.out_dir = os.path.join(args.out_dir, f'{args.exp_name}')\n",
    "args.vq_dir= f'output/{args.vq_name}'\n",
    "codebook_dir = f'{args.vq_dir}/codebook/'\n",
    "os.makedirs(args.out_dir, exist_ok = True)\n",
    "os.makedirs(args.vq_dir, exist_ok = True)\n",
    "os.makedirs(codebook_dir, exist_ok = True)\n",
    "\n",
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate)\n",
    "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code, \n",
    "                                    embed_dim=args.embed_dim_gpt, \n",
    "                                    clip_dim=args.clip_dim, \n",
    "                                    block_size=args.block_size, \n",
    "                                    num_layers=args.num_layers, \n",
    "                                    n_head=args.n_head_gpt, \n",
    "                                    drop_out_rate=args.drop_out_rate, \n",
    "                                    fc_rate=args.ff_rate)\n",
    "args.resume_pth = '/home/epinyoan/git/MaskText2Motion/T2M-GPT/output/VQVAE/net_last.pth'\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "\n",
    "# args.resume_trans = '/home/epinyoan/git/MaskText2Motion/T2M-BD/output/TEMP/net_last.pth'\n",
    "# ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "# trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder = torch.nn.DataParallel(trans_encoder)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:01<00:00, 4416.84it/s]\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 2\n",
    "num_workers = 8\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t, num_workers=num_workers)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pkeep = .5\n",
    "batch = next(train_loader_iter)\n",
    "clip_text, target, m_tokens_len = batch\n",
    "target, m_tokens_len = target.cuda(), m_tokens_len.cuda()\n",
    "bs = target.shape[0]\n",
    "\n",
    "text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    \n",
    "feat_clip_text = clip_model(text).float()\n",
    "\n",
    "# [INFO] Swap input index\n",
    "if args.pkeep == -1:\n",
    "    proba = np.random.rand(1)[0]\n",
    "    mask = torch.bernoulli(proba * torch.ones(target.shape,\n",
    "                                                        device=target.device))\n",
    "else:\n",
    "    mask = torch.bernoulli(args.pkeep * torch.ones(target.shape,\n",
    "                                                        device=target.device))\n",
    "mask = mask.round().to(dtype=torch.int64)\n",
    "r_indices = torch.randint_like(target, args.nb_code)\n",
    "input_indices = mask*target+(1-mask)*r_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time step masking\n",
    "batch_size, max_len = target.shape[:2]\n",
    "mask_id = get_model(net).vqvae.num_code + 2\n",
    "rand_time = uniform((batch_size,), device = target.device)\n",
    "rand_mask_probs = cosine_schedule(rand_time)\n",
    "num_token_masked = (m_tokens_len * rand_mask_probs).round().clamp(min = 1)\n",
    "\n",
    "# batch_randperm = torch.rand((batch_size, max_len), device = target.device).argsort(dim = -1)\n",
    "seq_mask = generate_src_mask(max_len, m_tokens_len+1)\n",
    "batch_randperm = torch.rand((batch_size, max_len), device = target.device) - seq_mask.int()\n",
    "batch_randperm = batch_randperm.argsort(dim = -1)\n",
    "mask_token = batch_randperm < rearrange(num_token_masked, 'b -> b 1')\n",
    "\n",
    "masked_target = torch.where(mask_token, input=target, other=-1)\n",
    "masked_input_indices = torch.where(mask_token, mask_id, input_indices)\n",
    "\n",
    "cls_pred = trans_encoder(masked_input_indices, feat_clip_text)[:, 1:]\n",
    "cls_pred = cls_pred.contiguous()\n",
    "\n",
    "# check that the random applies only actual seq not the blank\n",
    "torch.equal((mask_token * seq_mask).sum(-1), mask_token.sum(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.4568, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(6.4568, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. xent and apply weight later\n",
    "loss_cls4 = F.cross_entropy(rearrange(cls_pred, 'b n c -> b c n'), masked_target, reduction = 'none')\n",
    "weights = mask_token / (mask_token.sum(-1).unsqueeze(-1) * mask_token.shape[0])\n",
    "loss_cls4 = (loss_cls4*weights).sum()\n",
    "loss_cls4, weights.sum()\n",
    "\n",
    "# 5. mask first and xent later. Since we need \"cls_pred_all_masked\" to calculate acc so use this version\n",
    "cls_pred_all_masked = torch.masked_select(cls_pred, mask_token.unsqueeze(-1)).view(-1, cls_pred.shape[-1])\n",
    "target_all_masked = torch.masked_select(target, mask_token)\n",
    "weight_all_masked = torch.masked_select(weights, mask_token)\n",
    "loss_cls5 = F.cross_entropy(cls_pred_all_masked, target_all_masked, reduction = 'none')\n",
    "loss_cls5 = (loss_cls5 * weight_all_masked).sum()\n",
    "loss_cls4, loss_cls5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Compute Acc. #########\n",
    "args.if_maxtest = True # [INFO] for deterministic testing\n",
    "probs = torch.softmax(cls_pred_all_masked, dim=-1)\n",
    "if args.if_maxtest:\n",
    "    _, cls_pred_index = torch.max(probs, dim=-1)\n",
    "else:\n",
    "    dist = Categorical(probs)\n",
    "    cls_pred_index = dist.sample()\n",
    "(cls_pred_index == target_all_masked).sum().item(), \n",
    "# cls_pred_index, target_all_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.4831, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(6.4831, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Loss without random mask \n",
    "\n",
    "###### Compute Batch Xent ######\n",
    "loss_ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "cb_idx_mask = generate_src_mask(target.shape[1], m_tokens_len+1)\n",
    "cls_pred_all_masked = torch.masked_select(cls_pred, cb_idx_mask.unsqueeze(-1)).view(-1, cls_pred.shape[-1])\n",
    "target_all_masked = torch.masked_select(target, cb_idx_mask)\n",
    "\n",
    "denom = torch.ones(*target.shape).cuda() * bs * (m_tokens_len+1).unsqueeze(-1)\n",
    "denom = torch.masked_select(denom, cb_idx_mask)\n",
    "\n",
    "loss_cls = loss_ce(cls_pred_all_masked, target_all_masked) / denom\n",
    "loss_cls = loss_cls.sum()\n",
    "\n",
    "##### 2nd batch method compute loss first select later but maybe not faster b/c need to compute more loss\n",
    "# T2M-GPT was working but BD got error: \"RuntimeError: CUDA error: device-side assert triggered\"\n",
    "##############################################################################\n",
    "# denom = torch.ones(*target.shape).cuda() * bs * (m_tokens_len+1).unsqueeze(-1)\n",
    "# print(cls_pred.shape, target.shape, denom.shape, cb_idx_mask.shape)\n",
    "# cls_pred.view(-1, cls_pred.shape[-1])\n",
    "# loss_cls2 = loss_ce(cls_pred.view(-1, cls_pred.shape[-1]), \n",
    "#                     target.view(-1)).view(*denom.shape)/denom\n",
    "# loss_cls2 = torch.masked_select(loss_cls2, cb_idx_mask).sum()\n",
    "\n",
    "\n",
    "##### 3rd compute xent bf weight #####\n",
    "loss_cls3 = F.cross_entropy(rearrange(cls_pred, 'b n c -> b c n'), target, reduction = 'none')\n",
    "weights = generate_src_mask(target.shape[1], m_tokens_len+1)\n",
    "weights = cb_idx_mask/(m_tokens_len+1).unsqueeze(-1) / cb_idx_mask.shape[0]\n",
    "loss_cls3 = (loss_cls3*weights).sum()\n",
    "loss_cls, loss_cls3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 1718.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 51]),\n",
       " tensor([51.,  5., 51., 51.,  9., 51., 51.,  1.], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Mask Motion in Eval ####################\n",
    "index_motion = torch.randint(0, 513, (8, 51)).cuda()\n",
    "blank_id = 512 # get_model(trans_encoder).num_vq\n",
    "index_motion[1, 5] = blank_id\n",
    "index_motion[1, 7] = blank_id\n",
    "index_motion[4, 9] = blank_id\n",
    "index_motion[0, 0] = blank_id\n",
    "index_motion[-1, 1] = blank_id\n",
    "\n",
    "# [INFO] 1. this get the last index of blank_id\n",
    "# pred_length = (index_motion == blank_id).int().argmax(1).float()\n",
    "# [INFO] 2. this get the first index of blank_id\n",
    "pred_length = (index_motion >= blank_id).int()\n",
    "pred_length = torch.topk(pred_length, k=1, dim=1).indices.squeeze().float()\n",
    "pred_length[pred_length==0] = index_motion.shape[1]\n",
    "\n",
    "index_motion.shape, pred_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 14.59it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00, 14.48it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00, 14.38it/s]\n",
      "100%|██████████| 18/18 [00:01<00:00, 14.31it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_clip_text = torch.rand([32, 512])\n",
    "index_motion = trans_encoder(feat_clip_text, False, type=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 0 :, FID. 86.9617, Diversity Real. 10.8835, Diversity. 2.1975, R_precision_real. [0.40625    0.65625    0.80555556], R_precision. [0.04861111 0.07986111 0.10763889], matching_score_real. 2.6350843376583524, matching_score_pred. 10.219061533610025\n",
      "./Test/FID 86.96173394508502 0\n",
      "./Test/Diversity 2.197524 0\n",
      "./Test/top1 0.04861111111111111 0\n",
      "./Test/top2 0.0798611111111111 0\n",
      "./Test/top3 0.1076388888888889 0\n",
      "./Test/matching_score 10.219061533610025 0\n",
      "--> --> \t FID Improved from 1000.00000 to 86.96173 !!!\n",
      "--> --> \t matching_score Improved from 100.00000 to 10.21906 !!!\n",
      "--> --> \t Diversity Improved from 100.00000 to 2.19752 !!!\n",
      "--> --> \t Top1 Improved from 0.0000 to 0.0486 !!!\n",
      "--> --> \t Top2 Improved from 0.0000 to 0.0799 !!!\n",
      "--> --> \t Top3 Improved from 0.0000 to 0.1076 !!!\n"
     ]
    }
   ],
   "source": [
    "pred_pose_eval, pose, m_length, clip_text, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = \\\n",
    "    eval_trans.evaluation_transformer(args.out_dir, \n",
    "        val_loader, \n",
    "        net, \n",
    "        trans_encoder, \n",
    "        logger, \n",
    "        writer, \n",
    "        0, \n",
    "        best_fid=1000, \n",
    "        best_iter=0, \n",
    "        best_div=100, \n",
    "        best_top1=0, \n",
    "        best_top2=0, \n",
    "        best_top3=0, \n",
    "        best_matching=100, \n",
    "        clip_model=clip_model, \n",
    "        eval_wrapper=eval_wrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M-GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f05662df8c243d4bdb466e50f63c608a46b0022d922c28868d9dc874ab6f06d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
