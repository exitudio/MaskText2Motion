{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import models.vqvae as vqvae\n",
    "import utils.losses as losses \n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_VQ, dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from exit.utils import generate_src_mask\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n",
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "args = Temp()\n",
    "args.dataname = args.dataset_name = 't2m'\n",
    "args.mu = 0.99\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "\n",
    "if args.dataname == 'kit' : \n",
    "    dataset_opt_path = './checkpoints/kit/Comp_v6_KLD005/opt.txt'  \n",
    "    args.nb_joints = 21\n",
    "else :\n",
    "    dataset_opt_path = './checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "    args.nb_joints = 22\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTVQGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:07<00:00, 2942.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 23384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 256\n",
    "args.down_t = 2\n",
    "args.window_size = -1\n",
    "train_loader = dataset_VQ.DATALoader(args.dataname,\n",
    "                                        args.batch_size,\n",
    "                                        window_size=args.window_size,\n",
    "                                        unit_length=2**args.down_t)\n",
    "train_loader_iter = dataset_VQ.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion_full, m_len = next(train_loader_iter)\n",
    "gt_motion_full = gt_motion_full.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.vqvae_trans import Encoder, Decoder, VQVAE_Transformer, vqvae_wrapper\n",
    "from models.quantize_cnn import QuantizeEMAReset, QuantizeEMAReset_mask\n",
    "gt_motion_full = gt_motion_full.cuda()\n",
    "m_len = m_len.cuda()\n",
    "\n",
    "nb_code=512\n",
    "code_dim=512\n",
    "vqvae_trans = VQVAE_Transformer(args, nb_code=nb_code, code_dim=code_dim)\n",
    "vqvae_trans = torch.nn.DataParallel(vqvae_trans)\n",
    "vqvae_trans.cuda()\n",
    "\n",
    "motion_dim = 251 if args.dataname == 'kit' else 263\n",
    "encoder = Encoder(motion_dim=motion_dim)\n",
    "encoder = torch.nn.DataParallel(encoder)\n",
    "encoder.cuda()\n",
    "decoder = Decoder(motion_dim=motion_dim)\n",
    "decoder = torch.nn.DataParallel(decoder)\n",
    "decoder.cuda()\n",
    "\"\"\n",
    "# 3 losses:\n",
    "\n",
    "# pred_motion, loss_commit, perplexity = vqvae_trans(gt_motion_full, m_len, quantizer, type='full')\n",
    "# loss_commit = loss_commit.view(args.batch_size, -1, loss_commit.shape[-1])\n",
    "# target = vqvae_trans(gt_motion_full, m_len, type='encode')\n",
    "# pred_motion = vqvae_trans(target, torch.ceil(m_len/4), type='decode')\n",
    "# pred_motion.shape, loss_commit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12544, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = 4\n",
    "motion = gt_motion_full\n",
    "quantizer = QuantizeEMAReset(nb_code, code_dim, args)\n",
    "\n",
    "x_e = encoder(motion, m_len).permute(0,2,1)\n",
    "x_d, loss_commit, perplexity  = quantizer(x_e)\n",
    "x_d = x_d.permute(0,2,1)\n",
    "m_token_len = torch.ceil(m_len/patch_size)\n",
    "pred_motion = decoder(x_d, m_token_len)\n",
    "loss_commit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = gt_motion_full\n",
    "quantizer = QuantizeEMAReset_mask(nb_code, code_dim, args)\n",
    "patch_size = 4\n",
    "vqvae = vqvae_wrapper(patch_size, encoder, decoder, quantizer)\n",
    "pred_motion, x_d, loss_commit, perplexity = vqvae(gt_motion, m_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9387, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_commit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 196]) torch.Size([256, 196])\n"
     ]
    }
   ],
   "source": [
    "args.commit = 0.02\n",
    "args.loss_vel = 0.5\n",
    "args.recons_loss = 'l1_smooth'\n",
    "Loss = losses.ReConsLoss(args.recons_loss, args.nb_joints)\n",
    "\n",
    "\n",
    "\n",
    "m_token_len = torch.ceil(m_len/patch_size)\n",
    "max_token_len = math.ceil(gt_motion_full.shape[1]/patch_size)\n",
    "seq_mask = generate_src_mask(gt_motion_full.shape[1], m_len)\n",
    "seq_token_mask = generate_src_mask(max_token_len, m_token_len)\n",
    "\n",
    "# motion loss\n",
    "loss_motion = Loss(pred_motion, gt_motion_full).mean(-1)\n",
    "loss_vel = Loss.forward_vel(pred_motion, gt_motion_full).mean(-1)\n",
    "\n",
    "weights = seq_mask / (seq_mask.sum(1).unsqueeze(-1) * seq_mask.shape[0])\n",
    "weights = torch.masked_select(weights, seq_mask)\n",
    "loss_motion = torch.masked_select(loss_motion, seq_mask)\n",
    "loss_vel = torch.masked_select(loss_vel, seq_mask)\n",
    "\n",
    "\n",
    "# token loss\n",
    "loss_commit = loss_commit.mean(-1)\n",
    "token_weights = seq_token_mask / (seq_token_mask.sum(1).unsqueeze(-1) * seq_token_mask.shape[0])\n",
    "token_weights = torch.masked_select(token_weights, seq_token_mask)\n",
    "# loss_commit = torch.masked_select(loss_commit, seq_token_mask)\n",
    "\n",
    "# all loss\n",
    "loss_motion = (loss_motion*weights).sum()\n",
    "loss_vel = (loss_vel*weights).sum()\n",
    "loss_commit = (loss_commit * token_weights).sum()\n",
    "loss = loss_motion + args.loss_vel * loss_vel + args.commit * loss_commit\n",
    "# loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_weights.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2M-GPT (Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:07<00:00, 3056.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 20942\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 2\n",
    "args.down_t = 2\n",
    "args.window_size = 64\n",
    "train_loader = dataset_VQ.DATALoader(args.dataname,\n",
    "                                        args.batch_size,\n",
    "                                        window_size=args.window_size,\n",
    "                                        unit_length=2**args.down_t)\n",
    "train_loader_iter = dataset_VQ.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.nb_code = 512 # 8192 # \n",
    "args.code_dim = 512 # 32 # \n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.vq_act = 'relu'\n",
    "args.vq_norm = None\n",
    "\n",
    "args.quantizer = 'ema_reset'\n",
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate,\n",
    "                       args.vq_act,\n",
    "                       args.vq_norm)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "net.train()\n",
    "net.cuda()\n",
    "\n",
    "args.recons_loss = 'l1_smooth'\n",
    "args.nb_joints\n",
    "Loss = losses.ReConsLoss(args.recons_loss, args.nb_joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 31])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### DEBUG \n",
    "# temp = torch.rand(256, 64, 251).cuda()\n",
    "# pred_motion, loss_commit, perplexity = net(temp)\n",
    "# pred_motion, loss_commit, perplexity = net(gt_motion)\n",
    "# print(pred_motion.shape, loss_commit.shape, perplexity.shape)\n",
    "\n",
    "# net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "#                        args.nb_code,\n",
    "#                        args.code_dim,\n",
    "#                        args.output_emb_width,\n",
    "#                        args.down_t,\n",
    "#                        args.stride_t,\n",
    "#                        args.width,\n",
    "#                        args.depth,\n",
    "#                        args.dilation_growth_rate,\n",
    "#                        args.vq_act,\n",
    "#                        args.vq_norm)\n",
    "# net.cuda()\n",
    "# from torchsummary import summary\n",
    "# summary(net.vqvae.encoder, (251, 64))\n",
    "\n",
    "conv1 = torch.nn.Conv1d(in_channels=263, out_channels=512, \n",
    "                       kernel_size=3, \n",
    "                       stride=1, \n",
    "                       padding=1)\n",
    "conv2 = torch.nn.Conv1d(in_channels=512, out_channels=512, \n",
    "                       kernel_size=3, \n",
    "                       stride=1, \n",
    "                       padding=9,\n",
    "                       dilation=9)\n",
    "conv3 = torch.nn.Conv1d(in_channels=512, out_channels=512, \n",
    "                       kernel_size=1, \n",
    "                       stride=1)\n",
    "conv4 = torch.nn.Conv1d(in_channels=512, out_channels=512, \n",
    "                       kernel_size=4, \n",
    "                       stride=2)\n",
    "x = torch.rand(2, 263, 64)\n",
    "x = conv1(x)\n",
    "x = conv2(x)\n",
    "x = conv3(x)\n",
    "x = conv4(x)\n",
    "x.shape\n",
    "# (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
    "# (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
    "# (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([2, 512, 32])\n",
      " torch.Size([2, 512, 16])\n",
      " torch.Size([2, 512, 16])\n"
     ]
    }
   ],
   "source": [
    "args.commit = 0.02\n",
    "args.loss_vel = 0.5\n",
    "\n",
    "gt_motion = next(train_loader_iter)\n",
    "gt_motion = gt_motion.cuda().float() # (bs, 64, dim)\n",
    "\n",
    "pred_motion, loss_commit, perplexity = net(gt_motion)\n",
    "loss_motion = Loss(pred_motion, gt_motion)\n",
    "loss_vel = Loss.forward_vel(pred_motion, gt_motion)\n",
    "\n",
    "# loss = loss_motion + args.commit * loss_commit + args.loss_vel * loss_vel\n",
    "# gt_motion.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 2909.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False,\n",
    "                                        32,\n",
    "                                        w_vectorizer,\n",
    "                                        unit_length=2**args.down_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "./Test/FID fid nb_iter\n"
     ]
    }
   ],
   "source": [
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "logger.info('test')\n",
    "writer = LoggerWriterMock()\n",
    "writer.add_scalar('./Test/FID', 'fid', 'nb_iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:05<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 1 :, FID. 32.5747, Diversity Real. 9.8223, Diversity. 2.8842, R_precision_real. [0.51928191 0.68949468 0.78390957], R_precision. [0.03989362 0.07047872 0.10039894], matching_score_real. 2.9016788969648646, matching_score_pred. 7.219740370486645\n",
      "./Test/FID 32.57471793192871 1\n",
      "./Test/Diversity 2.8841934 1\n",
      "./Test/top1 0.0398936170212766 1\n",
      "./Test/top2 0.07047872340425532 1\n",
      "./Test/top3 0.10039893617021277 1\n",
      "./Test/matching_score 7.219740370486645 1\n",
      "--> --> \t Diversity Improved from 1.00000 to 2.88419 !!!\n"
     ]
    }
   ],
   "source": [
    "args.out_dir = './'\n",
    "a = eval_trans.evaluation_vqvae(\n",
    "    args.out_dir, \n",
    "    val_loader, \n",
    "    vqvae, #net, \n",
    "    logger, \n",
    "    writer, \n",
    "    nb_iter = 1, \n",
    "    best_fid = 1, \n",
    "    best_iter = 1, \n",
    "    best_div = 1, \n",
    "    best_top1 = 1, \n",
    "    best_top2 = 1, \n",
    "    best_top3 = 1, \n",
    "    best_matching = 1, \n",
    "    eval_wrapper=eval_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = next(iter(val_loader))\n",
    "motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
